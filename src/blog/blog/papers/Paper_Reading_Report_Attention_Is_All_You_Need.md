# 論文読解レポート：『Attention Is All You Need』

**プロジェクト名**: Transformerモデルの実装と検証
**日付**: 2025年9月2日

### 1. 概要

**1.1. 目的**
自然言語処理における基礎技術であるTransformerアーキテクチャへの理解を深めるため、論文『Attention Is All You Need』のモデルをゼロから実装する。実装モデルを実データセットで訓練し、機械翻訳タスクを遂行できることを通じて、理論と実践を結びつけることを目的とする。

**1.2. 期間**
2025年8月15日～2025年9月2日

**1.3. 成果**
Encoder-DecoderモデルであるTransformerをPyTorchで実装し、GPU環境での訓練に成功した。 訓練済みモデルが、未知の日本語文に対して英語の文章を生成できることを確認し、プロジェクトの目的を達成した。

---
### 2. 実装アーキテクチャ

実装は原論文の定義に準拠し、以下のコンポーネントで構成される。

* **全体構造**: EncoderおよびDecoderを各1つ、最終出力層として線形層を1つ配置した。
* **主要コンポーネント**:
    * **Encoder**: `Embedding`層、`PositionalEncoding`層、4層の`EncoderLayer`で構成。
    * **Decoder**: `Embedding`層、`PositionalEncoding`層、4層の`DecoderLayer`で構成。
    * **EncoderLayer**: `Multi-Head Attention`（自己注意）と`Feed-Forward Network`をそれぞれ1つずつ含む。
    * **DecoderLayer**: `Masked Multi-Head Attention`、`Cross-Attention`、`Feed-Forward Network`をそれぞれ1つずつ含む。
* **主要ハイパーパラメータ**:
    * モデル次元数 (`d_model`): 256
    * レイヤー数 (`num_layers`): 4
    * ヘッド数 (`num_heads`): 8

---
### 3. 開発・検証プロセス

本プロジェクトは、以下のフェーズで実施された。

* **フェーズ1: 理論調査 (2025年8月15日 - 8月25日)**
    * 論文『Attention Is All You Need』を読解し、Transformerの動作原理とアーキテクチャを分析した。
    * 論文実装の再現をプロジェクトの具体的な目標として設定した。

* **フェーズ2: モデル実装 (2025年8月31日 - 9月1日)**
    * PyTorchを用いてEncoderおよびDecoder部分を実装し、モデルの基本構造を完成させた。
    * 開発の再現性と再利用性を高めるため、Jupyter Notebook上のコードを`model.py`としてモジュール化した。

* **フェーズ3: 環境構築における課題と対応 (2025年9月2日)**
    * **課題**: GPU(CUDA)環境での実行時、PyTorchと`torchtext`のバージョン非互換に起因する`OSError: [WinError 127]`が頻発した。
    * **原因特定**: 原因は、`torchtext`ライブラリの非推奨化と、ローカルのCUDAバージョン（12.9）との相性問題にあると特定した。
    * **解決策**: Condaで仮想環境を再構築し、安定版のPyTorch (CUDA 12.1対応)を導入。 `torchtext`への依存を完全に排除する方針でコードを修正し、問題を解決した。

* **フェーズ4: データ前処理とモデル訓練 (2025年9月2日)**
    * **データセット**: Tanaka Corpusの日英対訳データから10,000文を抽出して使用した。
    * **語彙構築**: `torchtext`への依存を排除するため、単語の頻度計算とID化を行う`Vocab`クラスを独自に実装した。
    * **訓練**: `main.py`スクリプトを用い、GPU(CUDA)環境で訓練を実行。 10エポックの学習を完了し、損失が`5.283`から正常に低下することを確認した。

* **フェーズ5: モデル検証 (2025年9月2日)**
    * 学習済みモデルの重みを`transformer_jp_en.pth`として保存。
    * 保存した重みを読み込み、対話的に翻訳を行う推論スクリプト`translate.py`を作成し、動作を検証した。

---
### 4. 結果と考察

**4.1. 定性的評価**
訓練済みモデルを用いて実施した翻訳テストの結果は以下の通りである。

* **入力 (日本語)**: `猫はかわいい`
* **モデル出力 (英語)**: `he is a good at home .`

**4.2. 考察**
出力された英文は意味論的には正しくない。 しかし、ランダムな文字列ではなく、文法的に成立しうる単語の連なりで文章形式を成している。 これは、限定的なデータセットと訓練期間にもかかわらず、モデルが言語の構造的特徴の学習を開始していることを示唆する。

---
### 5. 結論

本プロジェクトを通じ、Transformerモデルのアーキテクチャをゼロから実装し、データ前処理、訓練、推論に至る一連のパイプライン構築を完了した。 これにより、当初設定した目的は達成された。

開発過程で発生した環境構築に関するエラーへの対応を通じ、深層学習プロジェクトにおける実践的なデバッグスキル、およびライブラリの依存関係管理に関する知見を獲得した。